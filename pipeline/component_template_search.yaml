name: Hhsearch
description: Searches for protein templates
inputs:
- {name: project, type: String}
- {name: project_number, type: String}
- {name: region, type: String}
- {name: template_dbs, type: JsonArray}
- {name: mmcif_db, type: String}
- {name: obsolete_db, type: String}
- {name: max_template_date, type: String}
- {name: reference_databases, type: Dataset}
- {name: sequence, type: Dataset}
- {name: msa, type: Dataset}
- {name: n_cpu, type: Integer, default: '8', optional: true}
- {name: maxseq, type: Integer, default: '1000000', optional: true}
- {name: max_template_hits, type: Integer, default: '20', optional: true}
- {name: machine_type, type: String, default: c2-standard-8, optional: true}
- {name: boot_disk_size, type: Integer, default: '200', optional: true}
outputs:
- {name: template_hits, type: Dataset}
- {name: template_features, type: Dataset}
- {name: cls_logging, type: Artifact}
implementation:
  container:
    image: gcr.io/jk-mlops-dev/alphafold-components
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef hhsearch(\n    project: str,\n    project_number: str,\n \
      \   region: str,\n    template_dbs: list,\n    mmcif_db: str,\n    obsolete_db:\
      \ str,\n    max_template_date: str,\n    reference_databases: Input[Dataset],\n\
      \    sequence: Input[Dataset],\n    msa: Input[Dataset],\n    template_hits:\
      \ Output[Dataset],\n    template_features: Output[Dataset],\n    cls_logging:\
      \ Output[Artifact],\n    n_cpu:int=8,\n    maxseq:int=1_000_000,\n    max_template_hits:int=20,\
      \ \n    machine_type:str='c2-standard-8',\n    boot_disk_size:int=200,\n   \
      \ ):\n    \"\"\"Searches for protein templates \"\"\"\n\n    import logging\n\
      \    import os\n    import sys\n    import time\n\n    from alphafold.data import\
      \ parsers\n    from job_runner import CustomJob\n    from dsub_wrapper import\
      \ run_dsub_job\n\n    _ALPHAFOLD_RUNNER_IMAGE = 'gcr.io/jk-mlops-dev/alphafold-components'\n\
      \    _SCRIPT = '/scripts/alphafold_runners/hhsearch_runner.py'\n\n    logging.basicConfig(format='%(asctime)s\
      \ - %(message)s',\n                      level=logging.INFO, \n            \
      \          datefmt='%d-%m-%y %H:%M:%S',\n                      stream=sys.stdout)\n\
      \n    database_paths = [reference_databases.metadata[database]\n           \
      \           for database in template_dbs]\n    database_paths = ','.join(database_paths)\n\
      \n\n    nfs_server, nfs_root_path, mount_path, network_name = reference_databases.uri.split(',')\n\
      \    network = f'projects/{project_number}/global/networks/{network_name}' \
      \   \n    params = {\n        'INPUT_PATH': sequence.uri,\n        'OUTPUT_PATH':\
      \ msa.uri,\n        'DB_ROOT': mount_path,\n        'DB_PATH': reference_databases.metadata[database],\n\
      \        'N_CPU': n_cpu,\n        'MAXSEQ': maxseq\n    } \n    job_name = f'JACKHMMER_JOB_{time.strftime(\"\
      %Y%m%d_%H%M%S\")}'\n\n    t0 = time.time()\n    logging.info('Starting database\
      \ search...')\n    custom_job = CustomJob.from_script_in_container(\n      \
      \  display_name=job_name,\n        script_path=_SCRIPT,\n        container_uri=_ALPHAFOLD_RUNNER_IMAGE,\n\
      \        project=project,\n        location=region,\n        machine_type=machine_type,\n\
      \        boot_disk_size_gb=boot_disk_size,\n        nfs_server=nfs_server,\n\
      \        nfs_root_path=nfs_root_path,\n        mount_path=mount_path,\n    \
      \    env_variables=params,\n    )\n    custom_job.run(\n       network=network\n\
      \    )\n    t1 = time.time()\n    logging.info(f'Search completed. Elapsed time:\
      \ {t1-t0}')\n\n    with open(template_hits.path) as f:\n        hhr_str = f.read()\n\
      \    parsed_hhr = parsers.parse_hhr(hhr_str)\n    template_hits.metadata['data_format']\
      \ = 'hhr'\n    template_hits.metadata['num of hits'] = len(parsed_hhr)\n   \
      \ template_features.metadata['data_format'] = 'pkl' \n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - hhsearch

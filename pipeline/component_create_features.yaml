name: Aggregate features
description: Aggregates MSAs and template features to create model features
inputs:
- {name: sequence, type: Dataset}
- {name: msa1, type: Dataset}
- {name: msa2, type: Dataset}
- {name: msa3, type: Dataset}
- {name: template_features, type: Dataset}
outputs:
- {name: model_features, type: Dataset}
implementation:
  container:
    image: gcr.io/jk-mlops-dev/alphafold
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef aggregate_features(\n    sequence: Input[Dataset],\n    msa1:\
      \ Input[Dataset],\n    msa2: Optional[Input[Dataset]],\n    msa3: Optional[Input[Dataset]],\n\
      \    template_features: Input[Dataset],\n    model_features: Output[Dataset]):\n\
      \    \"\"\"Aggregates MSAs and template features to create model features \n\
      \n    In the prototype, we assume a fixed number of inputs, mirroring the sample\n\
      \    inference pipeline from DeepMind. When it comes to productionizing we should\n\
      \    consider \"dynamic\" at runtime or at least highly configurable at compile\
      \ time\n    component.\n\n    \"\"\"\n\n    import os\n    import logging\n\
      \    import numpy as np\n    import pickle\n\n\n    from alphafold.common import\
      \ residue_constants\n    from alphafold.data import msa_identifiers\n    from\
      \ alphafold.data import parsers\n    from alphafold.data import templates\n\n\
      \n\n    def _make_sequence_features(\n        sequence: str, description: str,\
      \ num_res: int) -> FeatureDict:\n        \"\"\"Constructs a feature dict of\
      \ sequence features.\"\"\"\n        features = {}\n        features['aatype']\
      \ = residue_constants.sequence_to_onehot(\n            sequence=sequence,\n\
      \            mapping=residue_constants.restype_order_with_x,\n            map_unknown_to_x=True)\n\
      \        features['between_segment_residues'] = np.zeros((num_res,), dtype=np.int32)\n\
      \        features['domain_name'] = np.array([description.encode('utf-8')],\n\
      \                                        dtype=np.object_)\n        features['residue_index']\
      \ = np.array(range(num_res), dtype=np.int32)\n        features['seq_length']\
      \ = np.array([num_res] * num_res, dtype=np.int32)\n        features['sequence']\
      \ = np.array([sequence.encode('utf-8')], dtype=np.object_)\n        return features\n\
      \n\n    def _make_msa_features(msas: Sequence[parsers.Msa]) -> FeatureDict:\n\
      \        \"\"\"Constructs a feature dict of MSA features.\"\"\"\n        if\
      \ not msas:\n            raise ValueError('At least one MSA must be provided.')\n\
      \n        int_msa = []\n        deletion_matrix = []\n        uniprot_accession_ids\
      \ = []\n        species_ids = []\n        seen_sequences = set()\n        for\
      \ msa_index, msa in enumerate(msas):\n            if not msa:\n            \
      \    raise ValueError(f'MSA {msa_index} must contain at least one sequence.')\n\
      \            for sequence_index, sequence in enumerate(msa.sequences):\n   \
      \             if sequence in seen_sequences:\n                    continue\n\
      \                seen_sequences.add(sequence)\n                int_msa.append(\n\
      \                    [residue_constants.HHBLITS_AA_TO_ID[res] for res in sequence])\n\
      \                deletion_matrix.append(msa.deletion_matrix[sequence_index])\n\
      \                identifiers = msa_identifiers.get_identifiers(\n          \
      \          msa.descriptions[sequence_index])\n                uniprot_accession_ids.append(\n\
      \                    identifiers.uniprot_accession_id.encode('utf-8'))\n   \
      \             species_ids.append(identifiers.species_id.encode('utf-8'))\n\n\
      \        num_res = len(msas[0].sequences[0])\n        num_alignments = len(int_msa)\n\
      \        features = {}\n        features['deletion_matrix_int'] = np.array(deletion_matrix,\
      \ dtype=np.int32)\n        features['msa'] = np.array(int_msa, dtype=np.int32)\n\
      \        features['num_alignments'] = np.array(\n            [num_alignments]\
      \ * num_res, dtype=np.int32)\n        features['msa_uniprot_accession_identifiers']\
      \ = np.array(\n            uniprot_accession_ids, dtype=np.object_)\n      \
      \  features['msa_species_identifiers'] = np.array(species_ids, dtype=np.object_)\n\
      \        return features\n\n\n    def _read_msa(msa: Input[Dataset]):\n    \
      \    msa = None\n        msa_path = msa.path,\n        msa_format = msa.metadata['data_format']\n\
      \        if os.path.exists(msa_path):\n            with open(msa_path) as f:\n\
      \                msa = f.read()\n            if msa_format == 'sto':\n     \
      \           msa = parsers.parse_stockholm(msa)\n            elif msa_format\
      \ == 'a3m':\n                msa = parsers.parse_a3m(msa)\n            else:\n\
      \                raise RuntimeError(f'Unsupported MSA format: {msa_format}')\
      \ \n        return msa\n\n    def _read_sequence(sequence: Input[Dataset]):\n\
      \n        sequence_path = sequence.path\n        sequence_format = sequence.metadata['data_format']\n\
      \n        if sequence_format != 'fasta':\n            raise RuntimeError(f'Unsupported\
      \ sequence format {sequence_format}')\n\n        with open(sequence_path) as\
      \ f:\n            sequence_str = f.read()\n        sequences, sequence_descs\
      \ = parsers.parse_fasta(sequence_str)\n        if len(sequences) != 1:\n   \
      \         raise ValueError(\n                f'More than one input sequence\
      \ found in {sequence_path}.')\n\n        return sequences[0], sequence_descs[0],\
      \ len(sequences[0])\n\n\n    def _read_template_features(template_featurs: Input[Dataset]):\n\
      \        template_features_path = template_features.path\n        with open(template_features_path,\
      \ 'rb') as f:\n            template_features = pickle.load(f)\n        return\
      \ template_features\n\n\n    # Create sequence features\n    seq, seq_desc,\
      \ num_res = _read_sequence(sequence) \n    sequence_features = _make_sequence_features(\n\
      \        sequence=seq,\n        description=seq_desc,\n        num_res=num_res)\n\
      \n    # Create MSA features\n    msas = []\n    msas.append(_read_msa(msa1))\n\
      \    msas.append(_read_msa(msa2))\n    msas.append(_read_msa(msa3))\n    if\
      \ not msas:\n        raise RuntimeError('No MSAs passed to the component')\n\
      \    msa_features = _make_msa_features(msas=msas)\n\n    # Create template features\n\
      \    template_features = _read_template_features(template_features)\n\n    model_features\
      \ = {\n        **sequence_features,\n        **msa_features,\n        **template_features\n\
      \    }\n\n    model_features_path = model_features.path\n    model_features.metadata['data_format']\
      \ = 'pkl'\n    with open(model_features_path, 'wb') as f:\n        pickle.dump(model_features,\
      \ f, protocol=4)\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - aggregate_features

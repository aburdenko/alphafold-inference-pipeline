{
  "pipelineSpec": {
    "components": {
      "comp-db-search": {
        "executorLabel": "exec-db-search",
        "inputDefinitions": {
          "parameters": {
            "database_paths": {
              "type": "STRING"
            },
            "datasets_disk_image": {
              "type": "STRING"
            },
            "input_path": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "search_tool": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_msa": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-db-search-2": {
        "executorLabel": "exec-db-search-2",
        "inputDefinitions": {
          "parameters": {
            "database_paths": {
              "type": "STRING"
            },
            "datasets_disk_image": {
              "type": "STRING"
            },
            "input_path": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "search_tool": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_msa": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-db-search-3": {
        "executorLabel": "exec-db-search-3",
        "inputDefinitions": {
          "parameters": {
            "database_paths": {
              "type": "STRING"
            },
            "datasets_disk_image": {
              "type": "STRING"
            },
            "input_path": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "search_tool": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_msa": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-db-search-4": {
        "executorLabel": "exec-db-search-4",
        "inputDefinitions": {
          "parameters": {
            "database_paths": {
              "type": "STRING"
            },
            "datasets_disk_image": {
              "type": "STRING"
            },
            "input_path": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "search_tool": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_msa": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-db-search": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "db_search"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef db_search(\n    project: str,\n    region: str,\n    datasets_disk_image: str,\n    database_paths: str,\n    input_path: str,\n    search_tool: str,\n    output_msa: Output[Artifact], \n    tool_options: dict=None)-> str:\n    \"\"\"Searches sequence databases using the specified tool.\n\n    This is a simple prototype using dsub to submit a Cloud Life Sciences pipeline.\n    We are using CLS as KFP does not support attaching pre-populated disks or premtible VMs.\n    GCSFuse does not perform well with tools like hhsearch or hhblits.\n\n    \"\"\"\n\n    import logging\n    import os\n\n    from alphafold_components import dsub_wrapper\n\n    # For a prototype we are hardcoding some values. Whe productionizing\n    # we can make them compile time or runtime parameters\n    # E.g. CPU type is important. HHBlits requires at least SSE2 instruction set\n    # Works better with AVX2. \n    # At runtime we could pass them as tool_options dictionary\n\n    _REFERENCE_DATASETS_IMAGE = \"https://www.googleapis.com/compute/v1/projects/jk-mlops-dev/global/images/jk-alphafold-datasets 3000\"\n    _TOOL_TO_SETTINGS_MAPPING = {\n       'jackhmmer': {\n           'MACHINE_TYPE': 'n1-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'N_CPU': 4,\n           'MAX_STO_SEQUENCES': '10_000',\n           'FILE_FORMAT': 'sto',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/msa_runner.py' \n       },\n       'hhblits': {\n           'MACHINE_TYPE': 'c2-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'N_CPU': 4,\n           'FILE_FORMAT': 'a3m',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/msa_runner.py' \n       },\n       'hhsearch': {\n           'MACHINE_TYPE': 'c2-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'MAXSEQ': '1_000_000',\n           'FILE_FORMAT': 'hhr',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/template_runner.py' \n       }\n    }\n\n    _OUTPUT_FILE_PREFIX = 'output'\n\n    if not search_tool in _TOOL_TO_SETTINGS_MAPPING.keys():\n        raise ValueError(f'Unsupported tool: {search_tool}')\n    # We should probably also do some checking whether a given tool, DB combination works\n\n    _DSUB_PROVIDER = 'google-cls-v2'\n    _LOG_INTERVAL = '30s'\n    _IMAGE = 'gcr.io/jk-mlops-dev/alphafold'\n\n\n    # This is a temporary hack till we find a better option for dsub logging location\n    # It would be great if we can access pipeline root directly\n    # If not we can always pass the location as a parameter \n    logging_gcs_path = output_msa.uri.split('/')[2:-1]\n    folders = '/'.join(logging_gcs_path)\n    logging_gcs_path = f'gs://{folders}/logging'\n    script = _TOOL_TO_SETTINGS_MAPPING[search_tool].pop('SCRIPT') \n\n    dsub_job = dsub_wrapper.DsubJob(\n        image=_IMAGE,\n        project=project,\n        region=region,\n        logging=logging_gcs_path,\n        provider=_DSUB_PROVIDER,\n        machine_type=_TOOL_TO_SETTINGS_MAPPING[search_tool].pop('MACHINE_TYPE'),\n        boot_disk_size=_TOOL_TO_SETTINGS_MAPPING[search_tool].pop('BOOT_DISK_SIZE'),\n        log_interval=_LOG_INTERVAL\n    )\n\n    inputs = {\n        'INPUT_PATH': input_path, \n    }\n    file_format = _TOOL_TO_SETTINGS_MAPPING[search_tool].pop('FILE_FORMAT')\n    output_path =  os.path.join(output_msa.uri, f'{_OUTPUT_FILE_PREFIX}.{file_format}')\n    outputs = {\n        'OUTPUT_PATH': output_path\n    }\n    env_vars = {\n        'PYTHONPATH': '/app/alphafold',\n        'DATABASE_PATHS': database_paths,\n        'MSA_TOOL': search_tool,\n    }\n    env_vars.update(_TOOL_TO_SETTINGS_MAPPING[search_tool])\n\n    if not datasets_disk_image:\n        datasets_disk_image = _REFERENCE_DATASETS_IMAGE\n\n    disk_mounts = {\n        'DATABASES_ROOT': datasets_disk_image \n    }\n\n    logging.info('Starting a dsub job')\n    # Right now this is a blocking call. In future we should implement\n    # a polling loop to periodically retrieve logs, stdout and stderr\n    # and push it Vertex\n    result = dsub_job.run_job(\n        script=script,\n        inputs=inputs,\n        outputs=outputs,\n        env_vars=env_vars,\n        disk_mounts=disk_mounts\n    )\n\n    logging.info('Job completed')\n    logging.info(f'Completion status {result.returncode}')\n    logging.info(f'Logs: {result.stdout}')\n\n    if result.returncode != 0:\n        raise RuntimeError('dsub job failed')\n\n    output_msa.metadata['file_format']=file_format\n\n    return output_path\n\n"
            ],
            "image": "gcr.io/jk-mlops-dev/alphafold-components"
          }
        },
        "exec-db-search-2": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "db_search"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef db_search(\n    project: str,\n    region: str,\n    datasets_disk_image: str,\n    database_paths: str,\n    input_path: str,\n    search_tool: str,\n    output_msa: Output[Artifact], \n    tool_options: dict=None)-> str:\n    \"\"\"Searches sequence databases using the specified tool.\n\n    This is a simple prototype using dsub to submit a Cloud Life Sciences pipeline.\n    We are using CLS as KFP does not support attaching pre-populated disks or premtible VMs.\n    GCSFuse does not perform well with tools like hhsearch or hhblits.\n\n    \"\"\"\n\n    import logging\n    import os\n\n    from alphafold_components import dsub_wrapper\n\n    # For a prototype we are hardcoding some values. Whe productionizing\n    # we can make them compile time or runtime parameters\n    # E.g. CPU type is important. HHBlits requires at least SSE2 instruction set\n    # Works better with AVX2. \n    # At runtime we could pass them as tool_options dictionary\n\n    _REFERENCE_DATASETS_IMAGE = \"https://www.googleapis.com/compute/v1/projects/jk-mlops-dev/global/images/jk-alphafold-datasets 3000\"\n    _TOOL_TO_SETTINGS_MAPPING = {\n       'jackhmmer': {\n           'MACHINE_TYPE': 'n1-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'N_CPU': 4,\n           'MAX_STO_SEQUENCES': '10_000',\n           'FILE_FORMAT': 'sto',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/msa_runner.py' \n       },\n       'hhblits': {\n           'MACHINE_TYPE': 'c2-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'N_CPU': 4,\n           'FILE_FORMAT': 'a3m',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/msa_runner.py' \n       },\n       'hhsearch': {\n           'MACHINE_TYPE': 'c2-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'MAXSEQ': '1_000_000',\n           'FILE_FORMAT': 'hhr',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/template_runner.py' \n       }\n    }\n\n    _OUTPUT_FILE_PREFIX = 'output'\n\n    if not search_tool in _TOOL_TO_SETTINGS_MAPPING.keys():\n        raise ValueError(f'Unsupported tool: {search_tool}')\n    # We should probably also do some checking whether a given tool, DB combination works\n\n    _DSUB_PROVIDER = 'google-cls-v2'\n    _LOG_INTERVAL = '30s'\n    _IMAGE = 'gcr.io/jk-mlops-dev/alphafold'\n\n\n    # This is a temporary hack till we find a better option for dsub logging location\n    # It would be great if we can access pipeline root directly\n    # If not we can always pass the location as a parameter \n    logging_gcs_path = output_msa.uri.split('/')[2:-1]\n    folders = '/'.join(logging_gcs_path)\n    logging_gcs_path = f'gs://{folders}/logging'\n    script = _TOOL_TO_SETTINGS_MAPPING[search_tool].pop('SCRIPT') \n\n    dsub_job = dsub_wrapper.DsubJob(\n        image=_IMAGE,\n        project=project,\n        region=region,\n        logging=logging_gcs_path,\n        provider=_DSUB_PROVIDER,\n        machine_type=_TOOL_TO_SETTINGS_MAPPING[search_tool].pop('MACHINE_TYPE'),\n        boot_disk_size=_TOOL_TO_SETTINGS_MAPPING[search_tool].pop('BOOT_DISK_SIZE'),\n        log_interval=_LOG_INTERVAL\n    )\n\n    inputs = {\n        'INPUT_PATH': input_path, \n    }\n    file_format = _TOOL_TO_SETTINGS_MAPPING[search_tool].pop('FILE_FORMAT')\n    output_path =  os.path.join(output_msa.uri, f'{_OUTPUT_FILE_PREFIX}.{file_format}')\n    outputs = {\n        'OUTPUT_PATH': output_path\n    }\n    env_vars = {\n        'PYTHONPATH': '/app/alphafold',\n        'DATABASE_PATHS': database_paths,\n        'MSA_TOOL': search_tool,\n    }\n    env_vars.update(_TOOL_TO_SETTINGS_MAPPING[search_tool])\n\n    if not datasets_disk_image:\n        datasets_disk_image = _REFERENCE_DATASETS_IMAGE\n\n    disk_mounts = {\n        'DATABASES_ROOT': datasets_disk_image \n    }\n\n    logging.info('Starting a dsub job')\n    # Right now this is a blocking call. In future we should implement\n    # a polling loop to periodically retrieve logs, stdout and stderr\n    # and push it Vertex\n    result = dsub_job.run_job(\n        script=script,\n        inputs=inputs,\n        outputs=outputs,\n        env_vars=env_vars,\n        disk_mounts=disk_mounts\n    )\n\n    logging.info('Job completed')\n    logging.info(f'Completion status {result.returncode}')\n    logging.info(f'Logs: {result.stdout}')\n\n    if result.returncode != 0:\n        raise RuntimeError('dsub job failed')\n\n    output_msa.metadata['file_format']=file_format\n\n    return output_path\n\n"
            ],
            "image": "gcr.io/jk-mlops-dev/alphafold-components"
          }
        },
        "exec-db-search-3": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "db_search"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef db_search(\n    project: str,\n    region: str,\n    datasets_disk_image: str,\n    database_paths: str,\n    input_path: str,\n    search_tool: str,\n    output_msa: Output[Artifact], \n    tool_options: dict=None)-> str:\n    \"\"\"Searches sequence databases using the specified tool.\n\n    This is a simple prototype using dsub to submit a Cloud Life Sciences pipeline.\n    We are using CLS as KFP does not support attaching pre-populated disks or premtible VMs.\n    GCSFuse does not perform well with tools like hhsearch or hhblits.\n\n    \"\"\"\n\n    import logging\n    import os\n\n    from alphafold_components import dsub_wrapper\n\n    # For a prototype we are hardcoding some values. Whe productionizing\n    # we can make them compile time or runtime parameters\n    # E.g. CPU type is important. HHBlits requires at least SSE2 instruction set\n    # Works better with AVX2. \n    # At runtime we could pass them as tool_options dictionary\n\n    _REFERENCE_DATASETS_IMAGE = \"https://www.googleapis.com/compute/v1/projects/jk-mlops-dev/global/images/jk-alphafold-datasets 3000\"\n    _TOOL_TO_SETTINGS_MAPPING = {\n       'jackhmmer': {\n           'MACHINE_TYPE': 'n1-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'N_CPU': 4,\n           'MAX_STO_SEQUENCES': '10_000',\n           'FILE_FORMAT': 'sto',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/msa_runner.py' \n       },\n       'hhblits': {\n           'MACHINE_TYPE': 'c2-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'N_CPU': 4,\n           'FILE_FORMAT': 'a3m',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/msa_runner.py' \n       },\n       'hhsearch': {\n           'MACHINE_TYPE': 'c2-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'MAXSEQ': '1_000_000',\n           'FILE_FORMAT': 'hhr',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/template_runner.py' \n       }\n    }\n\n    _OUTPUT_FILE_PREFIX = 'output'\n\n    if not search_tool in _TOOL_TO_SETTINGS_MAPPING.keys():\n        raise ValueError(f'Unsupported tool: {search_tool}')\n    # We should probably also do some checking whether a given tool, DB combination works\n\n    _DSUB_PROVIDER = 'google-cls-v2'\n    _LOG_INTERVAL = '30s'\n    _IMAGE = 'gcr.io/jk-mlops-dev/alphafold'\n\n\n    # This is a temporary hack till we find a better option for dsub logging location\n    # It would be great if we can access pipeline root directly\n    # If not we can always pass the location as a parameter \n    logging_gcs_path = output_msa.uri.split('/')[2:-1]\n    folders = '/'.join(logging_gcs_path)\n    logging_gcs_path = f'gs://{folders}/logging'\n    script = _TOOL_TO_SETTINGS_MAPPING[search_tool].pop('SCRIPT') \n\n    dsub_job = dsub_wrapper.DsubJob(\n        image=_IMAGE,\n        project=project,\n        region=region,\n        logging=logging_gcs_path,\n        provider=_DSUB_PROVIDER,\n        machine_type=_TOOL_TO_SETTINGS_MAPPING[search_tool].pop('MACHINE_TYPE'),\n        boot_disk_size=_TOOL_TO_SETTINGS_MAPPING[search_tool].pop('BOOT_DISK_SIZE'),\n        log_interval=_LOG_INTERVAL\n    )\n\n    inputs = {\n        'INPUT_PATH': input_path, \n    }\n    file_format = _TOOL_TO_SETTINGS_MAPPING[search_tool].pop('FILE_FORMAT')\n    output_path =  os.path.join(output_msa.uri, f'{_OUTPUT_FILE_PREFIX}.{file_format}')\n    outputs = {\n        'OUTPUT_PATH': output_path\n    }\n    env_vars = {\n        'PYTHONPATH': '/app/alphafold',\n        'DATABASE_PATHS': database_paths,\n        'MSA_TOOL': search_tool,\n    }\n    env_vars.update(_TOOL_TO_SETTINGS_MAPPING[search_tool])\n\n    if not datasets_disk_image:\n        datasets_disk_image = _REFERENCE_DATASETS_IMAGE\n\n    disk_mounts = {\n        'DATABASES_ROOT': datasets_disk_image \n    }\n\n    logging.info('Starting a dsub job')\n    # Right now this is a blocking call. In future we should implement\n    # a polling loop to periodically retrieve logs, stdout and stderr\n    # and push it Vertex\n    result = dsub_job.run_job(\n        script=script,\n        inputs=inputs,\n        outputs=outputs,\n        env_vars=env_vars,\n        disk_mounts=disk_mounts\n    )\n\n    logging.info('Job completed')\n    logging.info(f'Completion status {result.returncode}')\n    logging.info(f'Logs: {result.stdout}')\n\n    if result.returncode != 0:\n        raise RuntimeError('dsub job failed')\n\n    output_msa.metadata['file_format']=file_format\n\n    return output_path\n\n"
            ],
            "image": "gcr.io/jk-mlops-dev/alphafold-components"
          }
        },
        "exec-db-search-4": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "db_search"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef db_search(\n    project: str,\n    region: str,\n    datasets_disk_image: str,\n    database_paths: str,\n    input_path: str,\n    search_tool: str,\n    output_msa: Output[Artifact], \n    tool_options: dict=None)-> str:\n    \"\"\"Searches sequence databases using the specified tool.\n\n    This is a simple prototype using dsub to submit a Cloud Life Sciences pipeline.\n    We are using CLS as KFP does not support attaching pre-populated disks or premtible VMs.\n    GCSFuse does not perform well with tools like hhsearch or hhblits.\n\n    \"\"\"\n\n    import logging\n    import os\n\n    from alphafold_components import dsub_wrapper\n\n    # For a prototype we are hardcoding some values. Whe productionizing\n    # we can make them compile time or runtime parameters\n    # E.g. CPU type is important. HHBlits requires at least SSE2 instruction set\n    # Works better with AVX2. \n    # At runtime we could pass them as tool_options dictionary\n\n    _REFERENCE_DATASETS_IMAGE = \"https://www.googleapis.com/compute/v1/projects/jk-mlops-dev/global/images/jk-alphafold-datasets 3000\"\n    _TOOL_TO_SETTINGS_MAPPING = {\n       'jackhmmer': {\n           'MACHINE_TYPE': 'n1-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'N_CPU': 4,\n           'MAX_STO_SEQUENCES': '10_000',\n           'FILE_FORMAT': 'sto',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/msa_runner.py' \n       },\n       'hhblits': {\n           'MACHINE_TYPE': 'c2-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'N_CPU': 4,\n           'FILE_FORMAT': 'a3m',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/msa_runner.py' \n       },\n       'hhsearch': {\n           'MACHINE_TYPE': 'c2-standard-4',\n           'BOOT_DISK_SIZE': '200',\n           'MAXSEQ': '1_000_000',\n           'FILE_FORMAT': 'hhr',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/template_runner.py' \n       }\n    }\n\n    _OUTPUT_FILE_PREFIX = 'output'\n\n    if not search_tool in _TOOL_TO_SETTINGS_MAPPING.keys():\n        raise ValueError(f'Unsupported tool: {search_tool}')\n    # We should probably also do some checking whether a given tool, DB combination works\n\n    _DSUB_PROVIDER = 'google-cls-v2'\n    _LOG_INTERVAL = '30s'\n    _IMAGE = 'gcr.io/jk-mlops-dev/alphafold'\n\n\n    # This is a temporary hack till we find a better option for dsub logging location\n    # It would be great if we can access pipeline root directly\n    # If not we can always pass the location as a parameter \n    logging_gcs_path = output_msa.uri.split('/')[2:-1]\n    folders = '/'.join(logging_gcs_path)\n    logging_gcs_path = f'gs://{folders}/logging'\n    script = _TOOL_TO_SETTINGS_MAPPING[search_tool].pop('SCRIPT') \n\n    dsub_job = dsub_wrapper.DsubJob(\n        image=_IMAGE,\n        project=project,\n        region=region,\n        logging=logging_gcs_path,\n        provider=_DSUB_PROVIDER,\n        machine_type=_TOOL_TO_SETTINGS_MAPPING[search_tool].pop('MACHINE_TYPE'),\n        boot_disk_size=_TOOL_TO_SETTINGS_MAPPING[search_tool].pop('BOOT_DISK_SIZE'),\n        log_interval=_LOG_INTERVAL\n    )\n\n    inputs = {\n        'INPUT_PATH': input_path, \n    }\n    file_format = _TOOL_TO_SETTINGS_MAPPING[search_tool].pop('FILE_FORMAT')\n    output_path =  os.path.join(output_msa.uri, f'{_OUTPUT_FILE_PREFIX}.{file_format}')\n    outputs = {\n        'OUTPUT_PATH': output_path\n    }\n    env_vars = {\n        'PYTHONPATH': '/app/alphafold',\n        'DATABASE_PATHS': database_paths,\n        'MSA_TOOL': search_tool,\n    }\n    env_vars.update(_TOOL_TO_SETTINGS_MAPPING[search_tool])\n\n    if not datasets_disk_image:\n        datasets_disk_image = _REFERENCE_DATASETS_IMAGE\n\n    disk_mounts = {\n        'DATABASES_ROOT': datasets_disk_image \n    }\n\n    logging.info('Starting a dsub job')\n    # Right now this is a blocking call. In future we should implement\n    # a polling loop to periodically retrieve logs, stdout and stderr\n    # and push it Vertex\n    result = dsub_job.run_job(\n        script=script,\n        inputs=inputs,\n        outputs=outputs,\n        env_vars=env_vars,\n        disk_mounts=disk_mounts\n    )\n\n    logging.info('Job completed')\n    logging.info(f'Completion status {result.returncode}')\n    logging.info(f'Logs: {result.stdout}')\n\n    if result.returncode != 0:\n        raise RuntimeError('dsub job failed')\n\n    output_msa.metadata['file_format']=file_format\n\n    return output_path\n\n"
            ],
            "image": "gcr.io/jk-mlops-dev/alphafold-components"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "alphafold-inference"
    },
    "root": {
      "dag": {
        "tasks": {
          "db-search": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-db-search"
            },
            "inputs": {
              "parameters": {
                "database_paths": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "uniref90/uniref90.fasta"
                    }
                  }
                },
                "datasets_disk_image": {
                  "componentInputParameter": "datasets_disk_image"
                },
                "input_path": {
                  "componentInputParameter": "fasta_path"
                },
                "project": {
                  "componentInputParameter": "project"
                },
                "region": {
                  "componentInputParameter": "region"
                },
                "search_tool": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "jackhmmer"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Search Uniref"
            }
          },
          "db-search-2": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-db-search-2"
            },
            "inputs": {
              "parameters": {
                "database_paths": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "mgnify/mgy_clusters_2018_12.fa"
                    }
                  }
                },
                "datasets_disk_image": {
                  "componentInputParameter": "datasets_disk_image"
                },
                "input_path": {
                  "componentInputParameter": "fasta_path"
                },
                "project": {
                  "componentInputParameter": "project"
                },
                "region": {
                  "componentInputParameter": "region"
                },
                "search_tool": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "jackhmmer"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Search Mgnify"
            }
          },
          "db-search-3": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-db-search-3"
            },
            "inputs": {
              "parameters": {
                "database_paths": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "uniclust30/uniclust30_2018_08/uniclust30_2018_08,bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt"
                    }
                  }
                },
                "datasets_disk_image": {
                  "componentInputParameter": "datasets_disk_image"
                },
                "input_path": {
                  "componentInputParameter": "fasta_path"
                },
                "project": {
                  "componentInputParameter": "project"
                },
                "region": {
                  "componentInputParameter": "region"
                },
                "search_tool": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "hhblits"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Search Uniclust and BFD"
            }
          },
          "db-search-4": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-db-search-4"
            },
            "dependentTasks": [
              "db-search"
            ],
            "inputs": {
              "parameters": {
                "database_paths": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "pdb70/pdb70"
                    }
                  }
                },
                "datasets_disk_image": {
                  "componentInputParameter": "datasets_disk_image"
                },
                "input_path": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "db-search"
                  }
                },
                "project": {
                  "componentInputParameter": "project"
                },
                "region": {
                  "componentInputParameter": "region"
                },
                "search_tool": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "hhsearch"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Search Pdb"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "datasets_disk_image": {
            "type": "STRING"
          },
          "fasta_path": {
            "type": "STRING"
          },
          "project": {
            "type": "STRING"
          },
          "region": {
            "type": "STRING"
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.11"
  },
  "runtimeConfig": {
    "parameters": {
      "datasets_disk_image": {
        "stringValue": "https://www.googleapis.com/compute/v1/projects/jk-mlops-dev/global/images/jk-alphafold-datasets 3000"
      },
      "project": {
        "stringValue": "jk-mlops-dev"
      },
      "region": {
        "stringValue": "us-central1"
      }
    }
  }
}
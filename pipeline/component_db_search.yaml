name: Db search
description: Searches sequence databases using the specified tool.
inputs:
- {name: project, type: String}
- {name: region, type: String}
- {name: datasets_disk_image, type: String}
- {name: database_paths, type: String}
- {name: input_path, type: String}
- {name: search_tool, type: String}
- {name: tool_options, type: JsonObject, optional: true}
outputs:
- {name: output_msa, type: Artifact}
- {name: Output, type: String}
implementation:
  container:
    image: gcr.io/jk-mlops-dev/alphafold-components
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef db_search(\n    project: str,\n    region: str,\n    datasets_disk_image:\
      \ str,\n    database_paths: str,\n    input_path: str,\n    search_tool: str,\n\
      \    output_msa: Output[Artifact], \n    tool_options: dict=None)-> str:\n \
      \   \"\"\"Searches sequence databases using the specified tool.\n\n    This\
      \ is a simple prototype using dsub to submit a Cloud Life Sciences pipeline.\n\
      \    We are using CLS as KFP does not support attaching pre-populated disks\
      \ or premtible VMs.\n    GCSFuse does not perform well with tools like hhsearch\
      \ or hhblits.\n\n    \"\"\"\n\n    print(output_msa)\n    print(output_msa.name)\n\
      \    print(output_msa.uri)\n    print(output_msa.metadata)\n\n\n\n\n    import\
      \ logging\n    import os\n\n    from alphafold_components import dsub_wrapper\n\
      \n    # For a prototype we are hardcoding some values. Whe productionizing\n\
      \    # we can make them compile time or runtime parameters\n    # E.g. CPU type\
      \ is important. HHBlits requires at least SSE2 instruction set\n    # Works\
      \ better with AVX2. \n    # At runtime we could pass them as tool_options dictionary\n\
      \n    _REFERENCE_DATASETS_IMAGE = \"https://www.googleapis.com/compute/v1/projects/jk-mlops-dev/global/images/jk-alphafold-datasets\
      \ 3000\"\n    _TOOL_TO_SETTINGS_MAPPING = {\n       'jackhmmer': {\n       \
      \    'MACHINE_TYPE': 'n1-standard-4',\n           'BOOT_DISK_SIZE': '200',\n\
      \           'N_CPU': 4,\n           'MAX_STO_SEQUENCES': '10_000',\n       \
      \    'FILE_FORMAT': 'sto',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/msa_runner.py'\
      \ \n       },\n       'hhblits': {\n           'MACHINE_TYPE': 'c2-standard-4',\n\
      \           'BOOT_DISK_SIZE': '200',\n           'N_CPU': 4,\n           'FILE_FORMAT':\
      \ 'a3m',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/msa_runner.py'\
      \ \n       },\n       'hhsearch': {\n           'MACHINE_TYPE': 'c2-standard-4',\n\
      \           'BOOT_DISK_SIZE': '200',\n           'MAXSEQ': '1_000_000',\n  \
      \         'FILE_FORMAT': 'hhr',\n           'SCRIPT': '/scripts/alphafold_components/alphafold_runners/template_runner.py'\
      \ \n       }\n    }\n\n    _OUTPUT_FILE_PREFIX = 'output'\n\n    if not search_tool\
      \ in _TOOL_TO_SETTINGS_MAPPING.keys():\n        raise ValueError(f'Unsupported\
      \ tool: {search_tool}')\n    # We should probably also do some checking whether\
      \ a given tool, DB combination works\n\n    _DSUB_PROVIDER = 'google-cls-v2'\n\
      \    _LOG_INTERVAL = '30s'\n    _IMAGE = 'gcr.io/jk-mlops-dev/alphafold'\n\n\
      \n    # This is a temporary hack till we find a better option for dsub logging\
      \ location\n    # It would be great if we can access pipeline root directly\n\
      \    # If not we can always pass the location as a parameter \n    logging_gcs_path\
      \ = output_msa.uri.split('/')[2:-1]\n    folders = '/'.join(logging_gcs_path)\n\
      \    logging_gcs_path = f'gs://{folders}/logging'\n    script = _TOOL_TO_SETTINGS_MAPPING[search_tool].pop('SCRIPT')\
      \ \n\n    dsub_job = dsub_wrapper.DsubJob(\n        image=_IMAGE,\n        project=project,\n\
      \        region=region,\n        logging=logging_gcs_path,\n        provider=_DSUB_PROVIDER,\n\
      \        machine_type=_TOOL_TO_SETTINGS_MAPPING[search_tool].pop('MACHINE_TYPE'),\n\
      \        boot_disk_size=_TOOL_TO_SETTINGS_MAPPING[search_tool].pop('BOOT_DISK_SIZE'),\n\
      \        log_interval=_LOG_INTERVAL\n    )\n\n    inputs = {\n        'INPUT_PATH':\
      \ input_path, \n    }\n    file_format = _TOOL_TO_SETTINGS_MAPPING[search_tool].pop('FILE_FORMAT')\n\
      \    output_path =  os.path.join(output_msa.uri, f'{_OUTPUT_FILE_PREFIX}.{file_format}')\n\
      \    outputs = {\n        'OUTPUT_PATH': output_path\n    }\n    env_vars =\
      \ {\n        'PYTHONPATH': '/app/alphafold',\n        'DATABASE_PATHS': database_paths,\n\
      \        'MSA_TOOL': search_tool,\n    }\n    env_vars.update(_TOOL_TO_SETTINGS_MAPPING[search_tool])\n\
      \n    if not datasets_disk_image:\n        datasets_disk_image = _REFERENCE_DATASETS_IMAGE\n\
      \n    disk_mounts = {\n        'DATABASES_ROOT': datasets_disk_image \n    }\n\
      \n    logging.info('Starting a dsub job')\n    # Right now this is a blocking\
      \ call. In future we should implement\n    # a polling loop to periodically\
      \ retrieve logs, stdout and stderr\n    # and push it Vertex\n    result = dsub_job.run_job(\n\
      \        script=script,\n        inputs=inputs,\n        outputs=outputs,\n\
      \        env_vars=env_vars,\n        disk_mounts=disk_mounts\n    )\n\n    logging.info('Job\
      \ completed')\n    logging.info(f'Completion status {result.returncode}')\n\
      \    logging.info(f'Logs: {result.stdout}')\n\n    if result.returncode != 0:\n\
      \        raise RuntimeError('dsub job failed')\n\n    output_msa.metadata['file_format']=file_format\n\
      \n    return output_path\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - db_search

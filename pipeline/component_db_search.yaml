name: Db search
description: Searches sequence databases using the specified tool.
inputs:
- {name: project, type: String}
- {name: region, type: String}
- {name: database_list, type: JsonArray}
- {name: reference_databases, type: Dataset}
- {name: input_data, type: Dataset}
outputs:
- {name: output_data, type: Dataset}
- {name: cls_logging, type: Artifact}
implementation:
  container:
    image: gcr.io/jk-mlops-dev/alphafold-components
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.11' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef db_search(\n    project: str,\n    region: str,\n    database_list:\
      \ list,\n    reference_databases: Input[Dataset],\n    input_data: Input[Dataset],\n\
      \    output_data: Output[Dataset],\n    cls_logging: Output[Artifact] \n   \
      \ ):\n    \"\"\"Searches sequence databases using the specified tool.\n\n  \
      \  This is a simple prototype using dsub to submit a Cloud Life Sciences pipeline.\n\
      \    We are using CLS as KFP does not support attaching pre-populated disks\
      \ or premtible VMs.\n    GCSFuse does not perform well with tools like hhsearch\
      \ or hhblits.\n\n    \"\"\"\n\n\n    import logging\n    import os\n    import\
      \ sys\n\n    from dsub_wrapper import run_dsub_job\n\n    _UNIREF90 = 'uniref90'\n\
      \    _MGNIFY = 'mgnify'\n    _BFD = 'bfd'\n    _UNICLUST30 = 'uniclust30'\n\
      \    _PDB70 = 'pdb70'\n    _PDB_MMCIF = 'pdb_mmcif'\n    _PDB_OBSOLETE = 'pdb_obsolete'\n\
      \    _PDB_SEQRES = 'pdb_seqres'\n    _UNIPROT = 'uniprot'\n\n    _DSUB_PROVIDER\
      \ = 'google-cls-v2'\n    _LOG_INTERVAL = '30s'\n    _ALPHAFOLD_RUNNER_IMAGE\
      \ = 'gcr.io/jk-mlops-dev/alphafold'\n\n    _DEFAULT_FILE_PREFIX = 'datafile'\n\
      \n    # For a prototype we are hardcoding some values. Whe productionizing\n\
      \    # we can make them compile time or runtime parameters\n    # E.g. CPU type\
      \ is important. HHBlits requires at least SSE2 instruction set\n    # Works\
      \ better with AVX2. \n    # At runtime we could pass them as tool_options dictionary\n\
      \    logging.basicConfig(format='%(asctime)s - %(message)s',\n             \
      \         level=logging.INFO, \n                      datefmt='%d-%m-%y %H:%M:%S',\n\
      \                      stream=sys.stdout)\n\n    _TOOL_TO_SETTINGS_MAPPING =\
      \ {\n       'jackhmmer': {\n           'MACHINE_TYPE': 'n1-standard-4',\n  \
      \         'BOOT_DISK_SIZE': '200',\n           'N_CPU': 4,\n           'MAXSEQ':\
      \ '10_000',\n           'INPUT_DATA_FORMAT': 'fasta',\n           'OUTPUT_DATA_FORMAT':\
      \ 'sto',\n           'SCRIPT': '/scripts/alphafold_runners/db_search_runner.py'\
      \ \n       },\n       'hhblits': {\n           'MACHINE_TYPE': 'c2-standard-4',\n\
      \           'BOOT_DISK_SIZE': '200',\n           'N_CPU': 4,\n           'MAXSEQ':\
      \ '1_000_000',\n           'INPUT_DATA_FORMAT': 'fasta',\n           'OUTPUT_DATA_FORMAT':\
      \ 'a3m',\n           'SCRIPT': '/scripts/alphafold_runners/db_search_runner.py'\
      \ \n       },\n       'hhsearch': {\n           'MACHINE_TYPE': 'c2-standard-4',\n\
      \           'BOOT_DISK_SIZE': '200',\n           'N_CPU': 0, # Not setable for\
      \ hhsearch\n           'MAXSEQ': '1_000_000',\n           'INPUT_DATA_FORMAT':\
      \ 'sto',\n           'OUTPUT_DATA_FORMAT': 'hhr',\n           'SCRIPT': '/scripts/alphafold_runners/db_search_runner.py'\
      \ \n       }\n    }\n\n    # This is a temporary crude solution to map a the\
      \ list of databases to search\n    # to a search tool. In the prototype we assume\
      \ that the provided databases list \n    # can be searched with a single tool\n\
      \    _DATABASE_TO_TOOL_MAPPING = {\n        _UNIREF90: 'jackhmmer',\n      \
      \  _MGNIFY: 'jackhmmer',\n        _BFD: 'hhblits',\n        _UNICLUST30: 'hhsearch',\
      \ \n        _PDB70 : 'hhsearch',\n        _PDB_MMCIF: None, # to be determined\n\
      \        _PDB_OBSOLETE: None, # to be determined\n        _PDB_SEQRES: None,\
      \ # to be determined\n        _UNIPROT: None, # to be determined\n    }\n\n\
      \    tools = [_DATABASE_TO_TOOL_MAPPING[db] for db in database_list\n      \
      \        if _DATABASE_TO_TOOL_MAPPING[db]]\n\n    if (not tools) or (len(tools)\
      \ > 1):\n        raise RuntimeError(f'The database list {database_list} not\
      \ supported')\n    db_tool = tools[0]\n\n    disk_image = reference_databases.metadata['disk_image']\n\
      \    database_paths = [reference_databases.metadata[database]\n            \
      \          for database in database_list]\n    database_paths = ','.join(database_paths)\n\
      \n    output_data_format = _TOOL_TO_SETTINGS_MAPPING[db_tool]['OUTPUT_DATA_FORMAT']\n\
      \    output_data.metadata['data_format'] = output_data_format\n    output_path\
      \ = output_data.uri\n\n    job_params = [\n        '--machine-type', _TOOL_TO_SETTINGS_MAPPING[db_tool]['MACHINE_TYPE'],\n\
      \        '--boot-disk-size', _TOOL_TO_SETTINGS_MAPPING[db_tool]['BOOT_DISK_SIZE'],\n\
      \        '--logging', cls_logging.uri,\n        '--log-interval', _LOG_INTERVAL,\
      \ \n        '--image', _ALPHAFOLD_RUNNER_IMAGE,\n        '--env', f'PYTHONPATH=/app/alphafold',\n\
      \        '--mount', f'DB_ROOT={disk_image}',\n        '--input', f'INPUT_DATA={input_data.uri}',\n\
      \        '--output', f'OUTPUT_DATA={output_path}',\n        '--env', f'DB_TOOL={db_tool}',\n\
      \        '--env', f'DB_PATHS={database_paths}',\n        '--env', f'N_CPU={_TOOL_TO_SETTINGS_MAPPING[db_tool][\"\
      N_CPU\"]}',\n        '--env', f'INPUT_DATA_FORMAT={_TOOL_TO_SETTINGS_MAPPING[db_tool][\"\
      INPUT_DATA_FORMAT\"]}', \n        '--env', f'OUTPUT_DATA_FORMAT={_TOOL_TO_SETTINGS_MAPPING[db_tool][\"\
      OUTPUT_DATA_FORMAT\"]}', \n        '--env', f'MAXSEQ={_TOOL_TO_SETTINGS_MAPPING[db_tool][\"\
      MAXSEQ\"]}', \n        '--script', _TOOL_TO_SETTINGS_MAPPING[db_tool]['SCRIPT']\
      \ \n    ]\n\n    result = run_dsub_job(\n        provider=_DSUB_PROVIDER,\n\
      \        project=project,\n        regions=region,\n        params=job_params,\n\
      \    ) \n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - db_search
